{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bouab\\AppData\\Local\\Temp\\ipykernel_12880\\3929526215.py:15: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n"
     ]
    }
   ],
   "source": [
    "# Install selenium\n",
    "# ! pip install selenium\n",
    "# Import the libraries.\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import urllib\n",
    "import time\n",
    "import os\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
    "\n",
    "HOME = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create url variable containing the webpage for a Google image search.\n",
    "# url = (\"https://www.google.com/search?q={s}&tbm=isch&tbs=sur%3Afc&hl=en&ved=0CAIQpwVqFwoTCKCa1c6s4-oCFQAAAAAdAAAAABAC&biw=1251&bih=568\")\n",
    "# # Launch the browser and open the given url in the webdriver.\n",
    "# driver.get(url.format(s='Pets'))\n",
    "# # Scroll down the body of the web page and load the images.\n",
    "# driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "# time.sleep(5)\n",
    "# # Find the images.\n",
    "# imgResults = driver.find_elements(By.XPATH,\"//img[contains(@class,'Q4LuWd')]\")\n",
    "# # Access and store the scr list of image url's.\n",
    "# src = []\n",
    "# for img in imgResults:\n",
    "#     src.append(img.get_attribute('src'))\n",
    "# # Retrieve and download the images.\n",
    "# for i in range(10):    urllib.request.urlretrieve(str(src[i]),\"sample_data/pets{}.jpg\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images(driver: webdriver, class_name: str, search_query: str, file_location: str=None, limit: int=200, wait_time:int=30):\n",
    "    # first define url\n",
    "    url = (\"https://www.google.com/search?q={s}&tbm=isch&tbs=sur%3Afc&hl=en&ved=0CAIQpwVqFwoTCKCa1c6s4-oCFQAAAAAdAAAAABAC&biw=1251&bih=568\")\n",
    "    # set the wait time for the driver\n",
    "    driver.implicitly_wait(wait_time)\n",
    "    \n",
    "    try:\n",
    "        driver.get(url.format(s=search_query))\n",
    "\n",
    "        img_results = []        \n",
    "\n",
    "        for i in range(10):\n",
    "            # Scroll down the body of the web page and load the images.\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "            time.sleep(10)\n",
    "            # Find the images.\n",
    "            img_results.extend(driver.find_elements(By.XPATH,\"//img[contains(@class,'Q4LuWd')]\"))\n",
    "        \n",
    "        print(f\"found {len(img_results)} images for {search_query}\")\n",
    "\n",
    "        if file_location is None:\n",
    "            file_location = os.path.join(os.getcwd(), 'data_selenium', class_name)\n",
    "            os.makedirs(file_location, exist_ok=True)\n",
    "\n",
    "        \n",
    "        for i, image in enumerate(img_results):                \n",
    "            # certain images are not associated with a link, which raises an error, adding a try catch block here maximizes the number of scrapped images\n",
    "            try:\n",
    "                urllib.request.urlretrieve(str(image.get_attribute('src')), os.path.join(file_location, f'{search_query}_image_{i}.jpg'))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Something wrong went with scraping {search_query}\")\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [['кухонный шкаф напольный', 'кухонный шкаф навесной', 'кухонный шкаф икеа', 'кухонный шкаф новый',\n",
    "            'кухонный шкаф с выдвижными ящиками','кухонный шкаф пенал'], \n",
    "           ['bathtub', 'bathtub new', 'bathtub modern', 'bathtub big', 'bathtub small'],\n",
    "           ['electrical switch on a wall', 'electrical switch white', 'electrical switch new', 'light switches','light switches on a wall'], \n",
    "           ['electrical socket', 'electrical socket on a wall', 'electrical outlet', 'electrical outlet on a wall', 'outlet for charger'], \n",
    "           ['sink', 'sink modern', 'sink new', 'sink big', 'sink kitchen'], \n",
    "           ['door', 'door new', 'door modern', 'door big', 'door small'],\n",
    "           ['toilet seat', 'toilet seat modern', 'toilet seat small', 'toilet seat new', 'toilet seat big']]\n",
    "\n",
    "\n",
    "classes = ['kitchen', 'bath', 'switch', 'socket', 'sink', 'door', 'toilet seat']\n",
    "\n",
    "# avoid running this piece of code unless the directo\n",
    "DATA_DIR = os.path.join(HOME, 'data_selenium')\n",
    "\n",
    "if not os.path.isdir(DATA_DIR) or len(os.listdir(os.path.join(HOME, 'data_selenium'))) == 0:\n",
    "    for c, qs in zip(classes[1:], queries[1:]):\n",
    "        for query in qs:\n",
    "            scrape_images(driver, c, search_query=query, wait_time=60)\n",
    "            print(f\"scrapping for {query} done !!!\")\n",
    "            print()\n",
    "        print(f\"SCRAPPING FOR CLASS {c} DONE!!!\")\n",
    "        print(\"\\n\"*3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data  has a large number of duplicate images, Manual filtering does not seem like a reasonable option taking into account the size of the scrapped data. Thus, image hasing comes to the rescue.   \n",
    "The Wavelet Hashing technique was used as it is does a better job detecting duplicates than the rest of the hashing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import imagehash as ih\n",
    "import numpy as np\n",
    "\n",
    "c = classes[-1]\n",
    "\n",
    "# first create the path variable\n",
    "dir_path = os.path.join(DATA_DIR, c)\n",
    "# get all the file names \n",
    "images = [os.path.join(dir_path, file_name) for file_name in os.listdir(dir_path)]\n",
    "i = images[0]\n",
    "h = ih.whash(Image.open(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to filter the images\n",
    "\n",
    "for c in classes:\n",
    "    # first create the path variable\n",
    "    dir_path = os.path.join(DATA_DIR, c)\n",
    "    # create a set to store the unique hashes\n",
    "    unique_hashes = set()\n",
    "    # iterate through each image\n",
    "    images = [os.path.join(dir_path, file_name) for file_name in os.listdir(dir_path)]\n",
    "    for i in images:\n",
    "        h = ih.whash(Image.open(i))\n",
    "        if h not in unique_hashes:\n",
    "            unique_hashes.add(h)\n",
    "        else:\n",
    "            # the image should be removed as it is a duplicated (or very similar to another existing image)\n",
    "            os.remove(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
